{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99857fbb",
   "metadata": {},
   "source": [
    "# WEB --> RAG PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3b3a8",
   "metadata": {},
   "source": [
    "User Query → Clean → Optimize → Web Search → Extract → Parse → Clean → Embed → Store in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d5ce61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best free vector databases for RAG projects in 2026',\n",
       " 'python faiss installation error windows fix',\n",
       " 'how transformers work in deep learning explained simply',\n",
       " 'iphone 15 vs samsung s24 battery performance test',\n",
       " 'symptoms of vitamin b12 deficiency in young adults',\n",
       " 'best mutual funds for long term investment india 2026',\n",
       " 'Budget trip plan for manali 3 days itinerary',\n",
       " 'roadmap to become data scientis after computer science degree',\n",
       " 'How to renew driving license online india steps',\n",
       " 'Open source llm models under 7b parameters for local use']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### USER QUERIES\n",
    "queries = [\n",
    "    \"best free vector databases for RAG projects in 2026\",\n",
    "    \"python faiss installation error windows fix\",\n",
    "    \"how transformers work in deep learning explained simply\",\n",
    "    \"iphone 15 vs samsung s24 battery performance test\",\n",
    "    \"symptoms of vitamin b12 deficiency in young adults\",\n",
    "    \"best mutual funds for long term investment india 2026\",\n",
    "    \"Budget trip plan for manali 3 days itinerary\",\n",
    "    \"roadmap to become data scientis after computer science degree\",\n",
    "    \"How to renew driving license online india steps\",\n",
    "    \"Open source llm models under 7b parameters for local use\"\n",
    "]\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872674a",
   "metadata": {},
   "source": [
    "Importing and downloading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5942c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\saras\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef7f11",
   "metadata": {},
   "source": [
    "# SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d812bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\Learning\\LLMs\\RAG\\.venv\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "Cleaning Progress: 100%|██████████| 39/39 [00:00<00:00, 3400.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'e',\n",
       " 'L',\n",
       " 'L',\n",
       " 'o',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_',\n",
       " '',\n",
       " 'M',\n",
       " 'y',\n",
       " '',\n",
       " 'N',\n",
       " 'a',\n",
       " 'm',\n",
       " 'e',\n",
       " '',\n",
       " 'I',\n",
       " 'S',\n",
       " '',\n",
       " 'J',\n",
       " 'A',\n",
       " 'i',\n",
       " '',\n",
       " 'S',\n",
       " 'A',\n",
       " 'r',\n",
       " 'a',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_',\n",
       " 's',\n",
       " 'w',\n",
       " 'A',\n",
       " 'T',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_',\n",
       " '_IS_PUNCT_']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy_cleaner\n",
    "from spacy_cleaner.processing import removers, replacers, mutators\n",
    "\n",
    "query = 'HeLLo!!??? My Name IS JAi SAra,.swAT,,!'\n",
    "\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "pipeline = spacy_cleaner.Cleaner(\n",
    "    model,\n",
    "    replacers.replace_punctuation_token,\n",
    ")\n",
    "pipeline.clean(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879b7ce",
   "metadata": {},
   "source": [
    "**Name-Entity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c708243b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine Learning', 'PERSON'), ('Artificial Intelligence', 'ORG')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"What is Machine Learning? What are it's types? How is it related to Artificial Intelligence?\"\n",
    "sent1_doc = model(sent1)\n",
    "[(token.text, token.label_) for token in sent1_doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1242eb0",
   "metadata": {},
   "source": [
    "**Noun Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25612d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'Machine Learning',\n",
       " 'What',\n",
       " \"it's types\",\n",
       " 'it',\n",
       " 'Artificial Intelligence']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chunk.text for chunk in sent1_doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8decce5",
   "metadata": {},
   "source": [
    "**Part-Of-Speech Mapping (POS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d59827a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 'PRON'),\n",
       " ('is', 'AUX'),\n",
       " ('Machine', 'PROPN'),\n",
       " ('Learning', 'PROPN'),\n",
       " ('?', 'PUNCT'),\n",
       " ('What', 'PRON'),\n",
       " ('are', 'AUX'),\n",
       " ('it', 'PRON'),\n",
       " (\"'s\", 'AUX'),\n",
       " ('types', 'NOUN'),\n",
       " ('?', 'PUNCT'),\n",
       " ('How', 'SCONJ'),\n",
       " ('is', 'AUX'),\n",
       " ('it', 'PRON'),\n",
       " ('related', 'VERB'),\n",
       " ('to', 'ADP'),\n",
       " ('Artificial', 'PROPN'),\n",
       " ('Intelligence', 'PROPN'),\n",
       " ('?', 'PUNCT')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.pos_) for token in sent1_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61d29cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'are', 'related']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in sent1_doc if token.head == token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5dc1379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_data(sent1_doc):    \n",
    "    return {\n",
    "    \"entities\": [(ent.text, ent.label_) for ent in sent1_doc.ents],\n",
    "    \"noun_chunks\": [chunk.text for chunk in sent1_doc.noun_chunks],\n",
    "    \"pos_tags\": [(token.text, token.pos_) for token in sent1_doc],\n",
    "    \"root_verbs\": [token.text for token in sent1_doc if token.head == token],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8981e44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [('Machine Learning', 'PERSON'),\n",
       "  ('Artificial Intelligence', 'ORG')],\n",
       " 'noun_chunks': ['What',\n",
       "  'Machine Learning',\n",
       "  'What',\n",
       "  \"it's types\",\n",
       "  'it',\n",
       "  'Artificial Intelligence'],\n",
       " 'pos_tags': [('What', 'PRON'),\n",
       "  ('is', 'AUX'),\n",
       "  ('Machine', 'PROPN'),\n",
       "  ('Learning', 'PROPN'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('What', 'PRON'),\n",
       "  ('are', 'AUX'),\n",
       "  ('it', 'PRON'),\n",
       "  (\"'s\", 'AUX'),\n",
       "  ('types', 'NOUN'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('How', 'SCONJ'),\n",
       "  ('is', 'AUX'),\n",
       "  ('it', 'PRON'),\n",
       "  ('related', 'VERB'),\n",
       "  ('to', 'ADP'),\n",
       "  ('Artificial', 'PROPN'),\n",
       "  ('Intelligence', 'PROPN'),\n",
       "  ('?', 'PUNCT')],\n",
       " 'root_verbs': ['is', 'are', 'related']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = semantic_data(sent1_doc)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae2f9a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"I'm\": 'I am',\n",
       " \"I'm'a\": 'I am about to',\n",
       " \"I'm'o\": 'I am going to',\n",
       " \"I've\": 'I have',\n",
       " \"I'll\": 'I will',\n",
       " \"I'll've\": 'I will have',\n",
       " \"I'd\": 'I would',\n",
       " \"I'd've\": 'I would have',\n",
       " 'Whatcha': 'What are you',\n",
       " \"amn't\": 'am not',\n",
       " \"ain't\": 'are not',\n",
       " \"aren't\": 'are not',\n",
       " \"'cause\": 'because',\n",
       " \"can't\": 'cannot',\n",
       " \"can't've\": 'cannot have',\n",
       " \"could've\": 'could have',\n",
       " \"couldn't\": 'could not',\n",
       " \"couldn't've\": 'could not have',\n",
       " \"daren't\": 'dare not',\n",
       " \"daresn't\": 'dare not',\n",
       " \"dasn't\": 'dare not',\n",
       " \"didn't\": 'did not',\n",
       " 'didn’t': 'did not',\n",
       " \"don't\": 'do not',\n",
       " 'don’t': 'do not',\n",
       " \"doesn't\": 'does not',\n",
       " \"e'er\": 'ever',\n",
       " \"everyone's\": 'everyone is',\n",
       " 'finna': 'fixing to',\n",
       " 'gimme': 'give me',\n",
       " \"gon't\": 'go not',\n",
       " 'gonna': 'going to',\n",
       " 'gotta': 'got to',\n",
       " \"hadn't\": 'had not',\n",
       " \"hadn't've\": 'had not have',\n",
       " \"hasn't\": 'has not',\n",
       " \"haven't\": 'have not',\n",
       " \"he've\": 'he have',\n",
       " \"he's\": 'he is',\n",
       " \"he'll\": 'he will',\n",
       " \"he'll've\": 'he will have',\n",
       " \"he'd\": 'he would',\n",
       " \"he'd've\": 'he would have',\n",
       " \"here's\": 'here is',\n",
       " \"how're\": 'how are',\n",
       " \"how'd\": 'how did',\n",
       " \"how'd'y\": 'how do you',\n",
       " \"how's\": 'how is',\n",
       " \"how'll\": 'how will',\n",
       " \"isn't\": 'is not',\n",
       " \"it's\": 'it is',\n",
       " \"'tis\": 'it is',\n",
       " \"'twas\": 'it was',\n",
       " \"it'll\": 'it will',\n",
       " \"it'll've\": 'it will have',\n",
       " \"it'd\": 'it would',\n",
       " \"it'd've\": 'it would have',\n",
       " 'kinda': 'kind of',\n",
       " \"let's\": 'let us',\n",
       " 'luv': 'love',\n",
       " \"ma'am\": 'madam',\n",
       " \"may've\": 'may have',\n",
       " \"mayn't\": 'may not',\n",
       " \"might've\": 'might have',\n",
       " \"mightn't\": 'might not',\n",
       " \"mightn't've\": 'might not have',\n",
       " \"must've\": 'must have',\n",
       " \"mustn't\": 'must not',\n",
       " \"mustn't've\": 'must not have',\n",
       " \"needn't\": 'need not',\n",
       " \"needn't've\": 'need not have',\n",
       " \"ne'er\": 'never',\n",
       " \"o'\": 'of',\n",
       " \"o'clock\": 'of the clock',\n",
       " \"ol'\": 'old',\n",
       " \"oughtn't\": 'ought not',\n",
       " \"oughtn't've\": 'ought not have',\n",
       " \"o'er\": 'over',\n",
       " \"shan't\": 'shall not',\n",
       " \"sha'n't\": 'shall not',\n",
       " \"shalln't\": 'shall not',\n",
       " \"shan't've\": 'shall not have',\n",
       " \"she's\": 'she is',\n",
       " \"she'll\": 'she will',\n",
       " \"she'd\": 'she would',\n",
       " \"she'd've\": 'she would have',\n",
       " \"should've\": 'should have',\n",
       " \"shouldn't\": 'should not',\n",
       " \"shouldn't've\": 'should not have',\n",
       " \"so've\": 'so have',\n",
       " \"so's\": 'so is',\n",
       " \"somebody's\": 'somebody is',\n",
       " \"someone's\": 'someone is',\n",
       " \"something's\": 'something is',\n",
       " 'sux': 'sucks',\n",
       " \"that're\": 'that are',\n",
       " \"that's\": 'that is',\n",
       " \"that'll\": 'that will',\n",
       " \"that'd\": 'that would',\n",
       " \"that'd've\": 'that would have',\n",
       " \"'em\": 'them',\n",
       " \"there're\": 'there are',\n",
       " \"there's\": 'there is',\n",
       " \"there'll\": 'there will',\n",
       " \"there'd\": 'there would',\n",
       " \"there'd've\": 'there would have',\n",
       " \"these're\": 'these are',\n",
       " \"they're\": 'they are',\n",
       " \"they've\": 'they have',\n",
       " \"they'll\": 'they will',\n",
       " \"they'll've\": 'they will have',\n",
       " \"they'd\": 'they would',\n",
       " \"they'd've\": 'they would have',\n",
       " \"this's\": 'this is',\n",
       " \"this'll\": 'this will',\n",
       " \"this'd\": 'this would',\n",
       " \"those're\": 'those are',\n",
       " \"to've\": 'to have',\n",
       " 'wanna': 'want to',\n",
       " \"wasn't\": 'was not',\n",
       " \"we're\": 'we are',\n",
       " \"we've\": 'we have',\n",
       " \"we'll\": 'we will',\n",
       " \"we'll've\": 'we will have',\n",
       " \"we'd\": 'we would',\n",
       " \"we'd've\": 'we would have',\n",
       " \"weren't\": 'were not',\n",
       " \"what're\": 'what are',\n",
       " \"what'd\": 'what did',\n",
       " \"what've\": 'what have',\n",
       " \"what's\": 'what is',\n",
       " \"what'll\": 'what will',\n",
       " \"what'll've\": 'what will have',\n",
       " \"when've\": 'when have',\n",
       " \"when's\": 'when is',\n",
       " \"where're\": 'where are',\n",
       " \"where'd\": 'where did',\n",
       " \"where've\": 'where have',\n",
       " \"where's\": 'where is',\n",
       " \"which's\": 'which is',\n",
       " \"who're\": 'who are',\n",
       " \"who've\": 'who have',\n",
       " \"who's\": 'who is',\n",
       " \"who'll\": 'who will',\n",
       " \"who'll've\": 'who will have',\n",
       " \"who'd\": 'who would',\n",
       " \"who'd've\": 'who would have',\n",
       " \"why're\": 'why are',\n",
       " \"why'd\": 'why did',\n",
       " \"why've\": 'why have',\n",
       " \"why's\": 'why is',\n",
       " \"will've\": 'will have',\n",
       " \"won't\": 'will not',\n",
       " \"won't've\": 'will not have',\n",
       " \"would've\": 'would have',\n",
       " \"wouldn't\": 'would not',\n",
       " \"wouldn't've\": 'would not have',\n",
       " \"y'all\": 'you all',\n",
       " \"y'all're\": 'you all are',\n",
       " \"y'all've\": 'you all have',\n",
       " \"y'all'd\": 'you all would',\n",
       " \"y'all'd've\": 'you all would have',\n",
       " \"you're\": 'you are',\n",
       " \"you've\": 'you have',\n",
       " \"you'll've\": 'you shall have',\n",
       " \"you'll\": 'you will',\n",
       " \"you'd\": 'you would',\n",
       " \"you'd've\": 'you would have',\n",
       " 'to cause': 'to cause',\n",
       " 'will cause': 'will cause',\n",
       " 'should cause': 'should cause',\n",
       " 'would cause': 'would cause',\n",
       " 'can cause': 'can cause',\n",
       " 'could cause': 'could cause',\n",
       " 'must cause': 'must cause',\n",
       " 'might cause': 'might cause',\n",
       " 'shall cause': 'shall cause',\n",
       " 'may cause': 'may cause',\n",
       " 'jan.': 'january',\n",
       " 'feb.': 'february',\n",
       " 'mar.': 'march',\n",
       " 'apr.': 'april',\n",
       " 'jun.': 'june',\n",
       " 'jul.': 'july',\n",
       " 'aug.': 'august',\n",
       " 'sep.': 'september',\n",
       " 'oct.': 'october',\n",
       " 'nov.': 'november',\n",
       " 'dec.': 'december',\n",
       " 'I’m': 'I am',\n",
       " 'I’m’a': 'I am about to',\n",
       " 'I’m’o': 'I am going to',\n",
       " 'I’ve': 'I have',\n",
       " 'I’ll': 'I will',\n",
       " 'I’ll’ve': 'I will have',\n",
       " 'I’d': 'I would',\n",
       " 'I’d’ve': 'I would have',\n",
       " 'amn’t': 'am not',\n",
       " 'ain’t': 'are not',\n",
       " 'aren’t': 'are not',\n",
       " '’cause': 'because',\n",
       " 'can’t': 'cannot',\n",
       " 'can’t’ve': 'cannot have',\n",
       " 'could’ve': 'could have',\n",
       " 'couldn’t': 'could not',\n",
       " 'couldn’t’ve': 'could not have',\n",
       " 'daren’t': 'dare not',\n",
       " 'daresn’t': 'dare not',\n",
       " 'dasn’t': 'dare not',\n",
       " 'doesn’t': 'does not',\n",
       " 'e’er': 'ever',\n",
       " 'everyone’s': 'everyone is',\n",
       " 'gon’t': 'go not',\n",
       " 'hadn’t': 'had not',\n",
       " 'hadn’t’ve': 'had not have',\n",
       " 'hasn’t': 'has not',\n",
       " 'haven’t': 'have not',\n",
       " 'he’ve': 'he have',\n",
       " 'he’s': 'he is',\n",
       " 'he’ll': 'he will',\n",
       " 'he’ll’ve': 'he will have',\n",
       " 'he’d': 'he would',\n",
       " 'he’d’ve': 'he would have',\n",
       " 'here’s': 'here is',\n",
       " 'how’re': 'how are',\n",
       " 'how’d': 'how did',\n",
       " 'how’d’y': 'how do you',\n",
       " 'how’s': 'how is',\n",
       " 'how’ll': 'how will',\n",
       " 'isn’t': 'is not',\n",
       " 'it’s': 'it is',\n",
       " '’tis': 'it is',\n",
       " '’twas': 'it was',\n",
       " 'it’ll': 'it will',\n",
       " 'it’ll’ve': 'it will have',\n",
       " 'it’d': 'it would',\n",
       " 'it’d’ve': 'it would have',\n",
       " 'let’s': 'let us',\n",
       " 'ma’am': 'madam',\n",
       " 'may’ve': 'may have',\n",
       " 'mayn’t': 'may not',\n",
       " 'might’ve': 'might have',\n",
       " 'mightn’t': 'might not',\n",
       " 'mightn’t’ve': 'might not have',\n",
       " 'must’ve': 'must have',\n",
       " 'mustn’t': 'must not',\n",
       " 'mustn’t’ve': 'must not have',\n",
       " 'needn’t': 'need not',\n",
       " 'needn’t’ve': 'need not have',\n",
       " 'ne’er': 'never',\n",
       " 'o’': 'of',\n",
       " 'o’clock': 'of the clock',\n",
       " 'ol’': 'old',\n",
       " 'oughtn’t': 'ought not',\n",
       " 'oughtn’t’ve': 'ought not have',\n",
       " 'o’er': 'over',\n",
       " 'shan’t': 'shall not',\n",
       " 'sha’n’t': 'shall not',\n",
       " 'shalln’t': 'shall not',\n",
       " 'shan’t’ve': 'shall not have',\n",
       " 'she’s': 'she is',\n",
       " 'she’ll': 'she will',\n",
       " 'she’d': 'she would',\n",
       " 'she’d’ve': 'she would have',\n",
       " 'should’ve': 'should have',\n",
       " 'shouldn’t': 'should not',\n",
       " 'shouldn’t’ve': 'should not have',\n",
       " 'so’ve': 'so have',\n",
       " 'so’s': 'so is',\n",
       " 'somebody’s': 'somebody is',\n",
       " 'someone’s': 'someone is',\n",
       " 'something’s': 'something is',\n",
       " 'that’re': 'that are',\n",
       " 'that’s': 'that is',\n",
       " 'that’ll': 'that will',\n",
       " 'that’d': 'that would',\n",
       " 'that’d’ve': 'that would have',\n",
       " '’em': 'them',\n",
       " 'there’re': 'there are',\n",
       " 'there’s': 'there is',\n",
       " 'there’ll': 'there will',\n",
       " 'there’d': 'there would',\n",
       " 'there’d’ve': 'there would have',\n",
       " 'these’re': 'these are',\n",
       " 'they’re': 'they are',\n",
       " 'they’ve': 'they have',\n",
       " 'they’ll': 'they will',\n",
       " 'they’ll’ve': 'they will have',\n",
       " 'they’d': 'they would',\n",
       " 'they’d’ve': 'they would have',\n",
       " 'this’s': 'this is',\n",
       " 'this’ll': 'this will',\n",
       " 'this’d': 'this would',\n",
       " 'those’re': 'those are',\n",
       " 'to’ve': 'to have',\n",
       " 'wasn’t': 'was not',\n",
       " 'we’re': 'we are',\n",
       " 'we’ve': 'we have',\n",
       " 'we’ll': 'we will',\n",
       " 'we’ll’ve': 'we will have',\n",
       " 'we’d': 'we would',\n",
       " 'we’d’ve': 'we would have',\n",
       " 'weren’t': 'were not',\n",
       " 'what’re': 'what are',\n",
       " 'what’d': 'what did',\n",
       " 'what’ve': 'what have',\n",
       " 'what’s': 'what is',\n",
       " 'what’ll': 'what will',\n",
       " 'what’ll’ve': 'what will have',\n",
       " 'when’ve': 'when have',\n",
       " 'when’s': 'when is',\n",
       " 'where’re': 'where are',\n",
       " 'where’d': 'where did',\n",
       " 'where’ve': 'where have',\n",
       " 'where’s': 'where is',\n",
       " 'which’s': 'which is',\n",
       " 'who’re': 'who are',\n",
       " 'who’ve': 'who have',\n",
       " 'who’s': 'who is',\n",
       " 'who’ll': 'who will',\n",
       " 'who’ll’ve': 'who will have',\n",
       " 'who’d': 'who would',\n",
       " 'who’d’ve': 'who would have',\n",
       " 'why’re': 'why are',\n",
       " 'why’d': 'why did',\n",
       " 'why’ve': 'why have',\n",
       " 'why’s': 'why is',\n",
       " 'will’ve': 'will have',\n",
       " 'won’t': 'will not',\n",
       " 'won’t’ve': 'will not have',\n",
       " 'would’ve': 'would have',\n",
       " 'wouldn’t': 'would not',\n",
       " 'wouldn’t’ve': 'would not have',\n",
       " 'y’all': 'you all',\n",
       " 'y’all’re': 'you all are',\n",
       " 'y’all’ve': 'you all have',\n",
       " 'y’all’d': 'you all would',\n",
       " 'y’all’d’ve': 'you all would have',\n",
       " 'you’re': 'you are',\n",
       " 'you’ve': 'you have',\n",
       " 'you’ll’ve': 'you shall have',\n",
       " 'you’ll': 'you will',\n",
       " 'you’d': 'you would',\n",
       " 'you’d’ve': 'you would have'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "contractions_dict = contractions.contractions_dict\n",
    "contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e750f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': [('Machine Learning', 'PERSON'),\n",
       "  ('Artificial Intelligence', 'ORG')],\n",
       " 'noun_chunks': ['What',\n",
       "  'Machine Learning',\n",
       "  'What',\n",
       "  \"it's types\",\n",
       "  'it',\n",
       "  'Artificial Intelligence'],\n",
       " 'pos_tags': [('What', 'PRON'),\n",
       "  ('is', 'AUX'),\n",
       "  ('Machine', 'PROPN'),\n",
       "  ('Learning', 'PROPN'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('What', 'PRON'),\n",
       "  ('are', 'AUX'),\n",
       "  ('it', 'PRON'),\n",
       "  (\"'s\", 'AUX'),\n",
       "  ('types', 'NOUN'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('How', 'SCONJ'),\n",
       "  ('is', 'AUX'),\n",
       "  ('it', 'PRON'),\n",
       "  ('related', 'VERB'),\n",
       "  ('to', 'ADP'),\n",
       "  ('Artificial', 'PROPN'),\n",
       "  ('Intelligence', 'PROPN'),\n",
       "  ('?', 'PUNCT')],\n",
       " 'root_verbs': ['is', 'are', 'related']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c78ea22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['What', 'is', 'Machine_Learning', '?'],\n",
       " ['What', 'are', \"it's_types\", '?'],\n",
       " ['How', 'is', 'it', 'related', 'to', 'Artificial_Intelligence', '?']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import TreebankWordTokenizer, sent_tokenize\n",
    "import re\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# --- Step 1: Prepare protected phrases ---\n",
    "entities = [e[0] for e in doc[\"entities\"]]\n",
    "noun_chunks = [c for c in doc[\"noun_chunks\"] if len(c.split()) > 1]\n",
    "\n",
    "protected_phrases = set(entities + noun_chunks)\n",
    "\n",
    "# --- Step 2: Mask phrases ---\n",
    "phrase_map = {}\n",
    "masked_text = sent1\n",
    "\n",
    "for i, phrase in enumerate(protected_phrases):\n",
    "    key = f\"__ENT{i}__\"\n",
    "    phrase_map[key] = phrase.replace(\" \", \"_\")  # join words\n",
    "    masked_text = re.sub(re.escape(phrase), key, masked_text)\n",
    "\n",
    "# --- Step 3: Sentence split ---\n",
    "sentences = sent_tokenize(masked_text)\n",
    "\n",
    "tokenized_query = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    # --- Step 4: Contraction expand ---\n",
    "    for word in sentence.split():\n",
    "        lw = word.lower()\n",
    "        if lw in contractions_dict:\n",
    "            temp.append(contractions_dict[lw])\n",
    "        else:\n",
    "            temp.append(word)\n",
    "\n",
    "    # --- Step 5: Tokenize ---\n",
    "    tokens = tokenizer.tokenize(\" \".join(temp))\n",
    "\n",
    "    # --- Step 6: Restore phrases ---\n",
    "    restored = [phrase_map.get(tok, tok) for tok in tokens]\n",
    "\n",
    "    tokenized_query.append(restored)\n",
    "\n",
    "tokenized_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9300c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = sent1\n",
    "semantics_data = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4510d9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"entities\": [\\n    [\\n      \"Machine Learning\",\\n      \"PERSON\"\\n    ],\\n    [\\n      \"Artificial Intelligence\",\\n      \"ORG\"\\n    ]\\n  ],\\n  \"noun_chunks\": [\\n    \"What\",\\n    \"Machine Learning\",\\n    \"What\",\\n    \"it\\'s types\",\\n    \"it\",\\n    \"Artificial Intelligence\"\\n  ],\\n  \"pos_tags\": [\\n    [\\n      \"What\",\\n      \"PRON\"\\n    ],\\n    [\\n      \"is\",\\n      \"AUX\"\\n    ],\\n    [\\n      \"Machine\",\\n      \"PROPN\"\\n    ],\\n    [\\n      \"Learning\",\\n      \"PROPN\"\\n    ],\\n    [\\n      \"?\",\\n      \"PUNCT\"\\n    ],\\n    [\\n      \"What\",\\n      \"PRON\"\\n    ],\\n    [\\n      \"are\",\\n      \"AUX\"\\n    ],\\n    [\\n      \"it\",\\n      \"PRON\"\\n    ],\\n    [\\n      \"\\'s\",\\n      \"AUX\"\\n    ],\\n    [\\n      \"types\",\\n      \"NOUN\"\\n    ],\\n    [\\n      \"?\",\\n      \"PUNCT\"\\n    ],\\n    [\\n      \"How\",\\n      \"SCONJ\"\\n    ],\\n    [\\n      \"is\",\\n      \"AUX\"\\n    ],\\n    [\\n      \"it\",\\n      \"PRON\"\\n    ],\\n    [\\n      \"related\",\\n      \"VERB\"\\n    ],\\n    [\\n      \"to\",\\n      \"ADP\"\\n    ],\\n    [\\n      \"Artificial\",\\n      \"PROPN\"\\n    ],\\n    [\\n      \"Intelligence\",\\n      \"PROPN\"\\n    ],\\n    [\\n      \"?\",\\n      \"PUNCT\"\\n    ]\\n  ],\\n  \"root_verbs\": [\\n    \"is\",\\n    \"are\",\\n    \"related\"\\n  ]\\n}'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(semantics_data, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99abf59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_semantic_data(semantic_data):\n",
    "    entities = list({e[0] for e in semantic_data[\"entities\"]})\n",
    "    noun_chunks = [c for c in semantic_data[\"noun_chunks\"] if len(c.split()) > 1]\n",
    "    root_verbs = semantic_data[\"root_verbs\"][:3]\n",
    "\n",
    "    return {\n",
    "        \"entities\": entities,\n",
    "        \"key_phrases\": noun_chunks[:5],\n",
    "        \"intent_verbs\": root_verbs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b03129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_json_array(text):\n",
    "    match = re.search(r'\\[.*\\]', text, re.S)\n",
    "    if match:\n",
    "        return json.loads(match.group())\n",
    "    return []\n",
    "\n",
    "def generate_query_variations(original_query, tokenized_query, semantic_data):\n",
    "\n",
    "    compact_semantic = compress_semantic_data(semantic_data)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "You are a Query Reformulation Engine.\n",
    "\n",
    "OUTPUT RULES:\n",
    "- Output ONLY a valid JSON array of exactly 5 strings.\n",
    "- No explanations.\n",
    "- No markdown.\n",
    "- No backticks.\n",
    "- Preserve named entities exactly.\n",
    "- Maintain original intent.\n",
    "- Do NOT introduce new topics.\n",
    "- Each query must be <= 20 words.\n",
    "- Make sure to divide the inten\n",
    "- If the original query contains multiple distinct questions or intents that cannot be naturally combined into one clear query, split them into separate focused queries instead of forcing a single long sentence.\n",
    "- However, still return exactly 5 total queries.\n",
    "- Each query must remain semantically equivalent to a part of the original intent.\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Original Query: {original_query}\n",
    "\n",
    "Semantic Anchors:\n",
    "{json.dumps(compact_semantic)}\n",
    "\n",
    "Task: Generate 5 equivalent search queries.\n",
    "Return ONLY JSON array.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openai/gpt-oss-120b\",\n",
    "        temperature=0.25,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    text = response.choices[0].message.content.strip()\n",
    "    # print(\"RAW LLM OUTPUT:\\n\", text)\n",
    "\n",
    "    try:\n",
    "        return extract_json_array(text)\n",
    "    except:\n",
    "        return [\"LLM_OUTPUT_PARSE_ERROR\", text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2187c163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is Machine Learning?', 'What are the types of Machine Learning?', 'How is Machine Learning related to Artificial Intelligence?', 'Define Machine Learning in the context of Artificial Intelligence.', 'List categories of Machine Learning and their connection to Artificial Intelligence.']\n"
     ]
    }
   ],
   "source": [
    "variations = generate_query_variations(\n",
    "    original_query=original_query,\n",
    "    tokenized_query=tokenized_query,\n",
    "    semantic_data=semantics_data\n",
    ")\n",
    "\n",
    "print(variations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce4afff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability import Document\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "import base64\n",
    "import time\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "# ---------- BING REDIRECT DECODER ----------\n",
    "def decode_bing_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "\n",
    "        # already real link\n",
    "        if \"bing.com\" not in parsed.netloc:\n",
    "            return url\n",
    "\n",
    "        qs = parse_qs(parsed.query)\n",
    "        if \"u\" not in qs:\n",
    "            return url\n",
    "\n",
    "        encoded = qs[\"u\"][0]\n",
    "        encoded = unquote(encoded)\n",
    "\n",
    "        # remove leading \"a1\"\n",
    "        if encoded.startswith(\"a1\"):\n",
    "            encoded = encoded[2:]\n",
    "\n",
    "        # fix base64 padding\n",
    "        encoded += \"=\" * (-len(encoded) % 4)\n",
    "\n",
    "        decoded = base64.b64decode(encoded).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "        if decoded.startswith(\"http\"):\n",
    "            return decoded\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Decode error:\", e)\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "# ---------- SEARCH USING BING HTML ----------\n",
    "def search_urls(query, max_results=5):\n",
    "    urls = []\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            \"https://www.bing.com/search\",\n",
    "            headers=HEADERS,\n",
    "            params={\"q\": query},\n",
    "            timeout=10\n",
    "        )\n",
    "\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "        for a in soup.select(\"li.b_algo h2 a\"):\n",
    "            link = a.get(\"href\")\n",
    "            if not link:\n",
    "                continue\n",
    "\n",
    "            link = decode_bing_url(link)\n",
    "\n",
    "            # skip tracking or bad links\n",
    "            if not link.startswith(\"http\") or \"bing.com\" in link:\n",
    "                continue\n",
    "\n",
    "            urls.append(link)\n",
    "\n",
    "            if len(urls) >= max_results:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Search error:\", e)\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "# ---------- FETCH HTML ----------\n",
    "def fetch_html(url, timeout=10):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r.text\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- EXTRACT MAIN TEXT ----------\n",
    "def extract_main_text(html):\n",
    "    try:\n",
    "        doc = Document(html)\n",
    "        summary_html = doc.summary(html_partial=True)\n",
    "\n",
    "        soup = BeautifulSoup(summary_html, \"lxml\")\n",
    "\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        text = soup.get_text(separator=\" \")\n",
    "        return \" \".join(text.split())\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ---------- NORMALIZE URL ----------\n",
    "def normalize_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    except:\n",
    "        return url\n",
    "\n",
    "\n",
    "# ---------- MAIN SCRAPER ----------\n",
    "def scrape_variations(variations, results_per_query=3, max_chars=6000):\n",
    "    seen_urls = set()\n",
    "    documents = []\n",
    "\n",
    "    for query in variations:\n",
    "        print(f\"\\nSearching: {query}\")\n",
    "        urls = search_urls(query, results_per_query)\n",
    "        print(\"URLs found:\", urls)\n",
    "\n",
    "        for url in urls:\n",
    "            nurl = normalize_url(url)\n",
    "            if nurl in seen_urls:\n",
    "                continue\n",
    "\n",
    "            seen_urls.add(nurl)\n",
    "            print(\"Fetching:\", nurl)\n",
    "\n",
    "            html = fetch_html(nurl)\n",
    "            if not html:\n",
    "                continue\n",
    "\n",
    "            text = extract_main_text(html)\n",
    "\n",
    "            # skip junk pages\n",
    "            if len(text) < 150:\n",
    "                continue\n",
    "\n",
    "            documents.append({\n",
    "                \"query\": query,\n",
    "                \"url\": nurl,\n",
    "                \"content\": text[:max_chars]\n",
    "            })\n",
    "\n",
    "            time.sleep(1)  # polite delay\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b469e421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching: What is Machine Learning?\n",
      "URLs found: ['https://research.ibm.com/topics/machine-learning', 'https://research.ibm.com/publications/quantum-machine-learning-an-interplay-between-quantum-computing-and-machine-learning', 'https://research.ibm.com/blog/ai-fairness-360']\n",
      "Fetching: https://research.ibm.com/topics/machine-learning\n",
      "Fetching: https://research.ibm.com/publications/quantum-machine-learning-an-interplay-between-quantum-computing-and-machine-learning\n",
      "Fetching: https://research.ibm.com/blog/ai-fairness-360\n",
      "\n",
      "Searching: What are the types of Machine Learning?\n",
      "URLs found: ['https://ask.csdn.net/questions/8894904', 'https://ask.csdn.net/questions/8233058', 'https://ask.csdn.net/questions/8293218']\n",
      "Fetching: https://ask.csdn.net/questions/8894904\n",
      "Fetching: https://ask.csdn.net/questions/8233058\n",
      "Fetching: https://ask.csdn.net/questions/8293218\n",
      "\n",
      "Searching: How is Machine Learning related to Artificial Intelligence?\n",
      "URLs found: ['https://www.geeksforgeeks.org/artificial-intelligence/machine-learning-versus-artificial-intelligence/', 'https://cloud.google.com/learn/artificial-intelligence-vs-machine-learning', 'https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks']\n",
      "Fetching: https://www.geeksforgeeks.org/artificial-intelligence/machine-learning-versus-artificial-intelligence/\n",
      "Fetching: https://cloud.google.com/learn/artificial-intelligence-vs-machine-learning\n",
      "Fetching: https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks\n",
      "\n",
      "Searching: Define Machine Learning in the context of Artificial Intelligence.\n",
      "URLs found: ['https://www.merriam-webster.com/dictionary/define', 'https://dictionary.cambridge.org/dictionary/english/definition', 'https://www.dictionary.com/browse/definition']\n",
      "Fetching: https://www.merriam-webster.com/dictionary/define\n",
      "Fetching: https://dictionary.cambridge.org/dictionary/english/definition\n",
      "Fetching: https://www.dictionary.com/browse/definition\n",
      "\n",
      "Searching: List categories of Machine Learning and their connection to Artificial Intelligence.\n",
      "URLs found: ['https://www.runoob.com/python/python-lists.html', 'https://www.runoob.com/python3/python3-list-operator.html', 'https://www.runoob.com/python3/python3-list.html']\n",
      "Fetching: https://www.runoob.com/python/python-lists.html\n",
      "Fetching: https://www.runoob.com/python3/python3-list-operator.html\n",
      "Fetching: https://www.runoob.com/python3/python3-list.html\n",
      "12\n",
      "https://research.ibm.com/publications/quantum-machine-learning-an-interplay-between-quantum-computing-and-machine-learning\n",
      "Quantum machine learning (QML) is a rapidly growing field that combines quantum computing principles with traditional machine learning. It seeks to revolutionize machine learning by harnessing the unique capabilities of quantum mechanics and employs machine learning techniques to advance quantum computing research. This paper presents an overview of quantum computing for the machine learning paradigm, where variational quantum circuits (VQC) are used to develop QML architectures on noisy interme\n",
      "Data written to scraped_documents.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "documents = scrape_variations(variations)\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[0][\"url\"])\n",
    "print(documents[0][\"content\"][:500])\n",
    "\n",
    "# ---- WRITE TO FILE ----\n",
    "with open(\"scraped_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Data written to scraped_documents.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
